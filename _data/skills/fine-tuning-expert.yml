id: fine-tuning-expert
slug: fine-tuning-expert
name: Fine-Tuning Expert
description: Hugging Face Transformers, PEFT library, bitsandbytes, LoRA/QLoRA, Axolotl,
  DeepSpeed, FSDP, instruction tuning, RLHF, DPO, dataset formatting (Alpaca, ShareGPT),
  evaluation (perplexity, BLEU, ROUGE),...
prompt_preview: "---\nname: fine-tuning-expert\ndescription: Use when fine-tuning\
  \ LLMs, training custom models, or optimizing model performance for specific tasks.\
  \ Invoke for parameter-efficient methods, dataset preparation, or model adaptation.\n\
  license: MIT\nmetadata:\n  author: https://github.com/Jeffallan\n  version: \"1.0.0\"\
  \n  domain: data-ml\n  triggers: fine-tuning, fine tuning, LoRA, QLoRA, PEFT, adapter\
  \ tuning, transfer learning, model training, custom model, LLM training, instruction\
  \ tuning, RLHF, model optimi..."
full_prompt_length: 3458
tools_mentioned: []
category: community
category_display: Community
source_repo: jeffallan/claude-skills
source_path: skills/fine-tuning-expert/SKILL.md
source_url: https://github.com/jeffallan/claude-skills/blob/main/skills/fine-tuning-expert/SKILL.md
fetched_at: '2026-02-15T04:22:03.695293+00:00'
evaluation:
  model: xiaomi/mimo-v2-flash:free
  evaluated_at: '2026-01-25T03:58:28.838846Z'
  prompt_quality:
    score: 4.5
    reasoning: The prompt is exceptionally well-structured with clear role definition,
      specific triggers, and comprehensive constraints. It follows best practices
      by defining scope, output format, and workflow. The only minor limitation is
      the reference system which assumes external files exist, but the core instructions
      remain actionable without them.
  usefulness:
    score: 5.0
    reasoning: This skill provides immediate practical value for ML engineers working
      with LLM fine-tuning. It covers the complete workflow from dataset preparation
      to deployment, includes specific methods like LoRA/QLoRA, and provides actionable
      constraints and templates. The real-world applicability is high for anyone training
      custom models or optimizing performance.
  overall_rating: 4.75
  summary: An excellent, production-ready skill prompt that comprehensively addresses
    LLM fine-tuning with clear structure, practical constraints, and complete workflow
    guidance for real-world ML engineering tasks.
  tags_suggested:
  - llm-fine-tuning
  - peft
  - lora
  - qlora
  - model-optimization
  - ml-engineering
github_metrics:
  stars: 0
  forks: 0
  open_issues: 0
  last_commit: null
  fetched_at: '2026-02-15T04:33:51.156886Z'
indexed_at: '2026-02-15T04:33:51.156892Z'
